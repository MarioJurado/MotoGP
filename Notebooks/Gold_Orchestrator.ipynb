{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import  col, input_file_name, monotonically_increasing_id, row_number, lit, current_timestamp, dayofmonth, dayofweek, dayofyear, month, year, weekofyear, date_format, concat_ws, sha2\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.window import Window\n",
        "from delta.tables import *\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parameters from pipeline\n",
        "years=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "year_list=ast.literal_eval(years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths Silver\n",
        "pathWSRSilver=\"abfss://silver@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/world-standing-riders/\"\n",
        "pathEventsSilver=\"abfss://silver@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/events/\"\n",
        "pathFRSilver=\"abfss://silver@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/full-results/\"\n",
        "\n",
        "# Paths Gold\n",
        "pathRaceFacts=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/races/\"\n",
        "pathEventsDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/events/\"\n",
        "pathRidersDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/riders/\"\n",
        "pathCircuitsDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/circuits/\"\n",
        "pathTeamsDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/teams/\"\n",
        "pathStandingsDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/standings/\"\n",
        "pathDateDim=\"abfss://gold@datalakemotogp.dfs.core.windows.net/api-racing-mike/motogp/date/\"\n",
        "\n",
        "# Gold Paths Dictionary\n",
        "paths_dict = {\n",
        "    \"pathRaceFacts\": pathRaceFacts,\n",
        "    \"pathEventsDim\": pathEventsDim,\n",
        "    \"pathRidersDim\": pathRidersDim,\n",
        "    \"pathCircuitsDim\": pathCircuitsDim,\n",
        "    \"pathTeamsDim\": pathTeamsDim,\n",
        "    \"pathStandingsDim\": pathStandingsDim\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Common functions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def add_load_date_to_df(df)->DataFrame:\n",
        "    if \"load_date\" not in df.columns:\n",
        "        df=df.withColumn(\"load_date\", current_timestamp())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Race Facts***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_race_facts():\n",
        "    df=spark.read.format(\"delta\").load(pathFRSilver)\n",
        "    dfRace=df.select(\"event_id\",\"result_id\",\"rider_id\",\"circuit_id\",\"team_id\",\\\n",
        "        \"position\",\"points\",\"time\",\"gap_first\",\"gap_lap\",\"total_laps\",\"average_speed\",\"rider_type\",\\\n",
        "        \"ground_condition\",\"humidity_condition\",\"weather_condition\",\"track_condition\",\"date\",\"month\",\"year\")\n",
        "    dfRace=dfRace.withColumn(\"pk_race_id\",sha2(concat_ws(\"_\", \"event_id\", \"result_id\", \"rider_id\", \"circuit_id\", \"team_id\"), 256))\\\n",
        "        .withColumnRenamed(\"event_id\",\"fk_event_id\").withColumnRenamed(\"result_id\",\"fk_result_id\").withColumnRenamed(\"rider_id\",\"fk_rider_id\")\\\n",
        "        .withColumnRenamed(\"circuit_id\",\"fk_circuit_id\").withColumnRenamed(\"team_id\",\"fk_team_id\")\n",
        "    dfRace=dfRace.dropDuplicates([\"pk_race_id\"])\n",
        "    dfRace=add_load_date_to_df(dfRace)\n",
        "    try:\n",
        "        race_gold=DeltaTable.forPath(spark,pathRaceFacts)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfRace.columns if col != \"load_date\"}\n",
        "        race_gold.alias(\"existing\").merge(dfRace.alias(\"new\"),\\\n",
        "            \"existing.pk_race_id=new.pk_race_id\")\\\n",
        "            .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfRace.write.format(\"delta\").save(pathRaceFacts)\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Event Dimension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_event_dim():\n",
        "    df=spark.read.format(\"delta\").load(pathEventsSilver)\n",
        "    dfEvents=df.select(\"event_id\",\"event_name\",\"short_name\",\"sponsored_name\",\"date_start\",\"date_end\",\"country_iso\",\"country_name\",\"year\")\n",
        "    dfEvents=dfEvents.withColumnRenamed(\"event_id\",\"pk_event_id\")\n",
        "    dfEvents=dfEvents.dropDuplicates([\"pk_event_id\"])\n",
        "    dfEvents=add_load_date_to_df(dfEvents)\n",
        "    try:\n",
        "        events_gold=DeltaTable.forPath(spark,pathEventsDim)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfEvents.columns if col != \"load_date\"}\n",
        "        events_gold.alias(\"existing\").merge(dfEvents.alias(\"new\"),\\\n",
        "            \"existing.pk_event_id = new.pk_event_id\")\\\n",
        "            .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfEvents.write.format(\"delta\").save(pathEventsDim)\n",
        "        else:\n",
        "            raise e  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Riders Dimension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_riders_dim():\n",
        "    df=spark.read.format(\"delta\").load(pathFRSilver)\n",
        "    dfRiders=df.select(\"rider_id\",\"rider_full_name\",\"rider_number\",\"rider_country_iso\",\"rider_country_name\",\"years_old\",\\\n",
        "        \"birth_date\",\"birth_city\",\"profile_picture_url\",\"portrait_picture_url\",\"number_picture_url\",\"helmet_picture_url\",\"country_flag_url\") \n",
        "    dfRiders=dfRiders.withColumnRenamed(\"rider_id\",\"pk_rider_id\")\n",
        "    dfRiders=dfRiders.dropDuplicates([\"pk_rider_id\"])\n",
        "    dfRiders=add_load_date_to_df(dfRiders)\n",
        "    try:\n",
        "        riders_gold=DeltaTable.forPath(spark,pathRidersDim)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfRiders.columns if col != \"load_date\"}\n",
        "        riders_gold.alias(\"existing\").merge(dfRiders.alias(\"new\"),\\\n",
        "            \"existing.pk_rider_id = new.pk_rider_id\")\\\n",
        "            .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfRiders.write.format(\"delta\").save(pathRidersDim)\n",
        "        else:\n",
        "            raise e "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Circuits Dimension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_circuits_dim():\n",
        "    df=spark.read.format(\"delta\").load(pathFRSilver)\n",
        "    dfCircuits=df.select(\"circuit_id\",\"circuit_name\",\"circuit_nation\",\"circuit_place\",\"circuit_country_iso\",\"circuit_country_name\")\n",
        "    dfCircuits=dfCircuits.withColumnRenamed(\"circuit_id\",\"pk_circuit_id\")\n",
        "    dfCircuits=dfCircuits.drop_duplicates([\"pk_circuit_id\"])\n",
        "    dfCircuits=add_load_date_to_df(dfCircuits)\n",
        "    try:\n",
        "        circuits_gold=DeltaTable.forPath(spark,pathCircuitsDim)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfCircuits.columns if col != \"load_date\"}\n",
        "        circuits_gold.alias(\"existing\").merge(dfCircuits.alias(\"new\"),\\\n",
        "            \"existing.pk_circuit_id = new.pk_circuit_id\")\\\n",
        "        .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfCircuits.write.format(\"delta\").save(pathCircuitsDim)\n",
        "        else:\n",
        "            raise e "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Teams Dimension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_teams_dim():\n",
        "    df=spark.read.format(\"delta\").load(pathFRSilver)\n",
        "    dfTeams=df.select(\"team_id\",\"team_name\",\"team_color\",\"sponsored_team\",\"constructor_name\",\"picture_url\",\"bike_picture_url\")\n",
        "    dfTeams=dfTeams.withColumnRenamed(\"team_id\",\"pk_team_id\")\n",
        "    dfTeams=dfTeams.dropDuplicates([\"pk_team_id\"])\n",
        "    dfTeams=add_load_date_to_df(dfTeams)\n",
        "    try:\n",
        "        teams_gold=DeltaTable.forPath(spark,pathTeamsDim)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfTeams.columns if col != \"load_date\"}\n",
        "        teams_gold.alias(\"existing\").merge(dfTeams.alias(\"new\"),\\\n",
        "            \"existing.pk_team_id = new.pk_team_id\")\\\n",
        "        .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfTeams.write.format(\"delta\").save(pathTeamsDim)\n",
        "        else:\n",
        "            raise e "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Date Dimension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_date_dim():\n",
        "    start_date=datetime(year_list[0],1,1)\n",
        "    end_date=datetime(year_list[-1],12,31)\n",
        "    date_array = [(start_date + timedelta(days=x)).strftime(\"%Y-%m-%d\") for x in range(0, (end_date-start_date).days + 1)]\n",
        "    df=spark.createDataFrame(date_array,\"string\").toDF(\"date\")\n",
        "    df=df.withColumn(\"date\",col(\"date\").cast(\"date\"))\n",
        "    df = df.withColumn(\"day\", dayofmonth(col(\"date\")))\n",
        "    df = df.withColumn(\"week\", weekofyear(col(\"date\")))\n",
        "    df = df.withColumn(\"month\", month(col(\"date\")))\n",
        "    df = df.withColumn(\"year\", year(col(\"date\")))\n",
        "    df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")))\n",
        "    df = df.withColumn(\"day_of_year\", dayofyear(col(\"date\")))\n",
        "    df = df.withColumn(\"quarter\", functions.quarter(col(\"date\")))\n",
        "    df = df.withColumn(\"week_day_name\", date_format(col(\"date\"), \"EEEE\"))\n",
        "    df = df.withColumn(\"month_name\", date_format(col(\"date\"), \"MMMM\")).orderBy(\"date\")\n",
        "    df=df.dropDuplicates([\"date\"])\n",
        "    try:\n",
        "        date_gold=DeltaTable.forPath(spark,pathDateDim)\n",
        "        date_gold.alias(\"existing\").merge(df.alias(\"new\"),\\\n",
        "            \"existing.date = new.date\")\\\n",
        "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            df.write.format(\"delta\").save(pathDateDim)\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Standing Facts***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#se debe ejecutar despues de races y  riders \n",
        "def process_standing_facts():\n",
        "    #WSR join with Riders to make Standing\n",
        "    df=spark.read.format(\"delta\").load(pathWSRSilver)\n",
        "    dfRiders=spark.read.format(\"delta\").load(pathRidersDim)\n",
        "    dfStand=df.alias(\"stand\").join(dfRiders.alias(\"riders\"),on=\"rider_full_name\",how=\"left\")\\\n",
        "        .select(\"stand.classification_id\",\"riders.pk_rider_id\",\"stand.position\",\"stand.points\",\"stand.year\")\n",
        "        \n",
        "    dfStand=dfStand.withColumnRenamed(\"classification_id\",\"pk_classification_id\").withColumnRenamed(\"pk_rider_id\",\"fk_rider_id\")\\\n",
        "        .drop(\"rider_full_name\")\n",
        "\n",
        "    #Standing join with Races to get that year's team_id (same Rider and same Year, gettin max teamid in case there was more than 1)\n",
        "    dfRace=spark.read.format(\"delta\").load(pathRaceFacts)\n",
        "    dfTPS=dfRace.groupBy(\"fk_rider_id\",\"year\").agg(functions.max(\"fk_team_id\").alias(\"team_id\"))\n",
        "    dfStand=dfStand.alias(\"stand\").join(dfTPS.alias(\"teams\"),(dfStand[\"fk_rider_id\"]==dfTPS[\"fk_rider_id\"]) & (dfStand[\"year\"]==dfTPS[\"year\"]),\n",
        "    how=\"inner\").select(\"stand.*\",\"teams.team_id\").withColumnRenamed(\"team_id\",\"fk_team_id\")\n",
        "\n",
        "    dfStand=dfStand.dropDuplicates([\"pk_classification_id\"])\n",
        "    dfStand=add_load_date_to_df(dfStand)\n",
        "    try:\n",
        "        standing_gold=DeltaTable.forPath(spark,pathStandingsDim)\n",
        "        update_columns = {col: f\"new.{col}\" for col in dfStand.columns if col != \"load_date\"}\n",
        "        standing_gold.alias(\"existing\").merge(dfStand.alias(\"new\"),\\\n",
        "            \"existing.pk_classification_id = new.pk_classification_id\")\\\n",
        "        .whenMatchedUpdate(set=update_columns).whenNotMatchedInsertAll().execute()\n",
        "    except Exception as e:\n",
        "        if 'not a Delta table' in str(e):\n",
        "            dfStand.write.format(\"delta\").save(pathStandingsDim)\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### ***Final process of facts and dimensions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "process_race_facts()\n",
        "process_event_dim()\n",
        "process_riders_dim()\n",
        "process_circuits_dim()\n",
        "process_teams_dim()\n",
        "process_date_dim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "process_standing_facts()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
